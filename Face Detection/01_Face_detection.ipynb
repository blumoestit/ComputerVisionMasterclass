{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invisible-collector",
   "metadata": {},
   "source": [
    "# OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-procedure",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-brunei",
   "metadata": {},
   "source": [
    "### Loading the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "involved-pickup",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "humanitarian-receiver",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/magdalenablum-oeste/Google Drive/GitHubMBO/ComputerVisionMasterclass/Face Detection\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "flexible-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "human-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Images/people1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "racial-lithuania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1920, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "parliamentary-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('image window', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-april",
   "metadata": {},
   "source": [
    "### Resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distant-ordinary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 800, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.resize(image, (800, 600))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "seven-enclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('image-resized window', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-visitor",
   "metadata": {},
   "source": [
    "### Image gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "theoretical-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('image gray', image_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "intimate-analyst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 800)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-sampling",
   "metadata": {},
   "source": [
    "## Detecting Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-dryer",
   "metadata": {},
   "source": [
    "### Cascade Classifier\n",
    "We use gray scale images to reduce the size of the image (from three to one channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-novelty",
   "metadata": {},
   "source": [
    "#### Load detector based on AdaBoost algorithm\n",
    "[Medium article](https://medium.com/neurosapiens/face-detectionface-11f77c9f4f3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "overhead-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "metric-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = face_detector.detectMultiScale(image_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "instructional-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115, 124,  53,  53],\n",
       "       [677,  72,  68,  68],\n",
       "       [475, 123,  59,  59],\n",
       "       [387, 233,  73,  73],\n",
       "       [ 92, 239,  66,  66],\n",
       "       [390, 323,  56,  56]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections\n",
    "# two first numbers are x, y coordinates of the recognized face\n",
    "# two last numbers show the size of the detected face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "coordinated-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detections) # each row indicate a face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-eclipse",
   "metadata": {},
   "source": [
    "#### Draw boxes around the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "shared-delicious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (x, y, w, h) in detections:\n",
    "    #print(x, y, w, h)\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 5)\n",
    "cv2.imshow('image gray', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-kruger",
   "metadata": {},
   "source": [
    "#### Add Haarcascade parameters\n",
    "This algorithm recognized one region as a face in the picture but it is not a face -> Add Haarcascade parameters: `scaleFactor`must be > 1 (depends on the image size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "excess-virgin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('Images/people1.jpg')\n",
    "image = cv2.resize(image, (800, 600))\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "detections = face_detector.detectMultiScale(image_gray, scaleFactor = 1.09)\n",
    "for (x, y, w, h) in detections:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0,255,0), 5)\n",
    "cv2.imshow('image fine detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-stanley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
